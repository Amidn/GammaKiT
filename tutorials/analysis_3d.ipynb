{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D analysis\n",
    "\n",
    "This tutorial shows how to run a stacked 3D map-based analysis using three example observations of the Galactic center region with CTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import EnergyDispersion, make_mean_psf, make_mean_edisp\n",
    "from gammapy.maps import WcsGeom, MapAxis, Map, WcsNDMap\n",
    "from gammapy.cube import MapMakerObs, PSFKernel, MapDataset\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    SkyDiffuseCube,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    PointSpatialModel,\n",
    ")\n",
    "from gammapy.spectrum import FluxPointsEstimator\n",
    "from gammapy.modeling import Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare modeling input data\n",
    "\n",
    "### Prepare input maps\n",
    "\n",
    "We first use the `DataStore` object to access the CTA observations and retrieve a list of observations by passing the observations IDs to the `.get_observations()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which data to use and print some information\n",
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/cta-1dc/index/gps/\")\n",
    "data_store.info()\n",
    "print(\n",
    "    \"Observation time (hours): \", data_store.obs_table[\"ONTIME\"].sum() / 3600\n",
    ")\n",
    "print(\"Observation table: \", data_store.obs_table.colnames)\n",
    "print(\"HDU table: \", data_store.hdu_table.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some observations from these dataset by hand\n",
    "obs_ids = [110380, 111140, 111159]\n",
    "observations = data_store.get_observations(obs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a reference geometry for our analysis, We choose a WCS based gemoetry with a binsize of 0.02 deg and also define an energy axis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_axis = MapAxis.from_edges(\n",
    "    np.logspace(-1.0, 1.0, 10), unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MapMaker` object is initialized with this reference geometry and a field of view cut of 4 deg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacked = MapDataset.create(geom=geom)\n",
    "\n",
    "for obs in observations:\n",
    "    maker = MapMakerObs(observation=obs, geom=geom, offset_max=4.0 * u.deg)\n",
    "    dataset = maker.run()\n",
    "    stacked.stack(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the stacked counts image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = stacked.counts.sum_over_axes()\n",
    "counts.smooth(width=\"0.05 deg\").plot(stretch=\"sqrt\", add_cbar=True, vmax=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the background image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = stacked.background_model.map.sum_over_axes()\n",
    "background.plot(stretch=\"sqrt\", add_cbar=True, vmax=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one the exposure image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = stacked.exposure.sum_over_axes()\n",
    "exposure.plot(stretch=\"sqrt\", add_cbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute an excess image just with  a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excess = counts - background\n",
    "excess.smooth(5).plot(stretch=\"sqrt\", add_cbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare IRFs\n",
    "\n",
    "To estimate the mean PSF across all observations at a given source position `src_pos`, we use `make_mean_psf()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean PSF\n",
    "src_pos = SkyCoord(0, 0, unit=\"deg\", frame=\"galactic\")\n",
    "table_psf = make_mean_psf(observations, src_pos)\n",
    "\n",
    "# PSF kernel used for the model convolution\n",
    "psf_kernel = PSFKernel.from_table_psf(table_psf, geom, max_radius=\"0.3 deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the mean energy dispersion across all observations at a given source position `src_pos`, we use `make_mean_edisp()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define energy grid\n",
    "energy = energy_axis.edges\n",
    "\n",
    "# mean edisp\n",
    "edisp = make_mean_edisp(\n",
    "    observations, position=src_pos, e_true=energy, e_reco=energy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save maps and IRFs to disk\n",
    "\n",
    "It is common to run the preparation step independent of the likelihood fit, because often the preparation of maps, PSF and energy dispersion is slow if you have a lot of data. We first create a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"analysis_3d\")\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then write the maps and IRFs to disk by calling the dedicated `.write()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset to file\n",
    "stacked.write(\"stacked-dataset.fits.gz\", overwrite=True)\n",
    "\n",
    "# write IRFs\n",
    "psf_kernel.write(str(path / \"psf.fits\"), overwrite=True)\n",
    "edisp.write(str(path / \"edisp.fits\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood fit\n",
    "\n",
    "### Reading maps and IRFs\n",
    "As first step we read in the maps and IRFs that we have saved to disk again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read maps\n",
    "maps = {\n",
    "    \"counts\": Map.read(str(path / \"counts.fits\")),\n",
    "    \"background\": Map.read(str(path / \"background.fits\")),\n",
    "    \"exposure\": Map.read(str(path / \"exposure.fits\")),\n",
    "}\n",
    "\n",
    "# read IRFs\n",
    "psf_kernel = PSFKernel.read(str(path / \"psf.fits\"))\n",
    "edisp = EnergyDispersion.read(str(path / \"edisp.fits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit mask\n",
    "\n",
    "To select a certain energy range for the fit we can create a fit mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = maps[\"counts\"].geom.get_coord()\n",
    "mask = coords[\"energy\"] > 0.3 * u.TeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit\n",
    "\n",
    "No we are ready for the actual likelihood fit. We first define the model as a combination of a point source with a powerlaw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=\"0.01 deg\", lat_0=\"0.01 deg\", frame=\"galactic\"\n",
    ")\n",
    "spectral_model = PowerLawSpectralModel(\n",
    "    index=2.2, amplitude=\"3e-12 cm-2 s-1 TeV-1\", reference=\"1 TeV\"\n",
    ")\n",
    "model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it is useful to fit the normalization (and also the tilt) of the background. To do so, we have to define the background as a model. In this example, we will keep the tilt fixed and the norm free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = BackgroundModel(maps[\"background\"], norm=1.1, tilt=0.0)\n",
    "background_model.parameters[\"norm\"].frozen = False\n",
    "background_model.parameters[\"tilt\"].frozen = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the `MapDataset` object by passing the prepared maps, IRFs as well as the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MapDataset(\n",
    "    model=model,\n",
    "    counts=maps[\"counts\"],\n",
    "    exposure=maps[\"exposure\"],\n",
    "    background_model=background_model,\n",
    "    mask_fit=mask,\n",
    "    psf=psf_kernel,\n",
    "    edisp=edisp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we run the model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit = Fit(dataset)\n",
    "result = fit.run(optimize_opts={\"print_level\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model fit\n",
    "\n",
    "We check the model fit by computing a residual image. For this we first get the number of predicted counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npred = dataset.npred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute a residual image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = maps[\"counts\"] - npred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual.sum_over_axes().smooth(width=0.05 * u.deg).plot(\n",
    "    cmap=\"coolwarm\", vmin=-1, vmax=1, add_cbar=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the best fit spectrum. For that need to extract the covariance of the spectral parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model.spectral_model\n",
    "\n",
    "# set covariance on the spectral model\n",
    "covariance = result.parameters.covariance\n",
    "spec.parameters.covariance = covariance[2:5, 2:5]\n",
    "\n",
    "energy_range = [0.3, 10] * u.TeV\n",
    "spec.plot(energy_range=energy_range, energy_power=2)\n",
    "spec.plot_error(energy_range=energy_range, energy_power=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently our model should be improved by adding a component for diffuse Galactic emission and at least one second point source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Galactic diffuse emission to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both models at the same time, our diffuse model (the same from the Fermi file used before) and our model for the central source. This time, in order to make it more realistic, we will consider an exponential cut off power law spectral model for the source. We will fit again the normalization and tilt of the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_model = SkyDiffuseCube.read(\n",
    "    \"$GAMMAPY_DATA/fermi-3fhl-gc/gll_iem_v06_gc.fits.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=\"-0.05 deg\", lat_0=\"-0.05 deg\", frame=\"galactic\"\n",
    ")\n",
    "spectral_model = ExpCutoffPowerLawSpectralModel(\n",
    "    index=2,\n",
    "    amplitude=3e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n",
    "    reference=1.0 * u.TeV,\n",
    "    lambda_=0.1 / u.TeV,\n",
    ")\n",
    "\n",
    "model_ecpl = SkyModel(\n",
    "    spatial_model=spatial_model,\n",
    "    spectral_model=spectral_model,\n",
    "    name=\"gc-source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combined = MapDataset(\n",
    "    model=model_ecpl + diffuse_model,\n",
    "    counts=maps[\"counts\"],\n",
    "    exposure=maps[\"exposure\"],\n",
    "    background_model=background_model,\n",
    "    psf=psf_kernel,\n",
    "    edisp=edisp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_combined = Fit(dataset_combined)\n",
    "result_combined = fit_combined.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have now two components in our model, and we can access them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking normalization value (the closer to 1 the better)\n",
    "print(model_ecpl, \"\\n\")\n",
    "print(background_model, \"\\n\")\n",
    "print(diffuse_model, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the normalization of the background has vastly improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the residual image considering this improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual2 = maps[\"counts\"] - dataset_combined.npred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a comparison, we can plot our previous residual map (left) and the new one (right) with the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "ax_1 = plt.subplot(121, projection=residual.geom.wcs)\n",
    "ax_2 = plt.subplot(122, projection=residual.geom.wcs)\n",
    "\n",
    "ax_1.set_title(\"Without diffuse emission subtraction\")\n",
    "ax_2.set_title(\"With diffuse emission subtraction\")\n",
    "\n",
    "residual.sum_over_axes().smooth(width=0.05 * u.deg).plot(\n",
    "    cmap=\"coolwarm\", vmin=-1, vmax=1, add_cbar=True, ax=ax_1\n",
    ")\n",
    "residual2.sum_over_axes().smooth(width=0.05 * u.deg).plot(\n",
    "    cmap=\"coolwarm\", vmin=-1, vmax=1, add_cbar=True, ax=ax_2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Flux Points\n",
    "\n",
    "Finally we compute flux points for the galactic center source. For this we first define an energy binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_edges = [0.3, 1, 3, 10] * u.TeV\n",
    "fpe = FluxPointsEstimator(\n",
    "    datasets=[dataset_combined], e_edges=e_edges, source=\"gc-source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flux_points = fpe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the best fit model and flux points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points.table[\"is_ul\"] = flux_points.table[\"ts\"] < 4\n",
    "ax = flux_points.plot(energy_power=2)\n",
    "model_ecpl.spectral_model.plot(\n",
    "    ax=ax, energy_range=energy_range, energy_power=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Note that this notebook aims to show you the procedure of a 3D analysis using just a few observations and a cutted Fermi model. Results get much better for a more complete analysis considering the GPS dataset from the CTA First Data Challenge (DC-1) and also the CTA model for the Galactic diffuse emission, as shown in the next image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/DC1_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete tutorial notebook of this analysis is available to be downloaded in [GAMMAPY-EXTRA](https://github.com/gammapy/gammapy-extra) repository at https://github.com/gammapy/gammapy-extra/blob/master/analyses/cta_1dc_gc_3d.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Analyse the second source in the field of view: G0.9+0.1 and add it to the combined model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
