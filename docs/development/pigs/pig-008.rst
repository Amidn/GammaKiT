.. include:: ../../references.txt

.. _pig-008:

****************
PIG 8 - Datasets
****************

* Author: Axel Donath (editor), Christoph Deil, Regis Terrier & Atreyee Sinha
* Created: Jan 4th, 2018
* Accepted:
* Status:
* Discussion:


Introduction
============
An essential feature of Gammapy for modeling gamma-ray data will be the possibility
of joint-likelihood analyses. This includes the joint-likelhood fitting of a model
across multiple observations, joint-likelihood fitting of IACT and Fermi-LAT
data, combined analysis of gamma-ray data with flux points or a combined spectral
and cube analysis. Joint-likelihood also allows for combining different event types
in a single analysis and timing analyses. For this reason we propose to introduce
the abstraction layer of `Dataset` in Gammapy. A dataset bundles the reduced data
with a parameteric model and fit statistics function. It evaluates the model and
log-likelihood and passes it on to the fit object. Datasets can be combined by
adding their log-likelihood values and concatenating their model parameters.


Proposal
========
We propose to introduce the following classes to implement the dataset handling
in Gammapy:


`MapDataset`
------------
To enable the standard combined spectral and spatial analysis we propose to
introduce a `MapDataset` class. A `MapDataset` bundles the counts data, source model,
IRFs, background model, corresponding to a given event selection.

It is supposed to work as following:

.. code ::

    from gammapy.cube import MapDataset

    model = SourceModel.read("model.yaml")
    background = BackgroundModel.read("stacked/background.fits")

    counts = Map.read("stacked/counts.fits")
    exposure = Map.read("stacked/exposure.fits")
    edisp = Map.read("stacked/edisp.fits")
    psf = Map.read("stacked/psf.fits")

    dataset = MapDataset(
        counts=counts,
        exposure=exposure,
        edisp=edisp,
        psf=psf,
        mask=mask,
        model=model,
        background=background_model,
        likelihood="cash"
        )

    fit = Fit(dataset)
    fit.optimize()

The edisp and psf maps can be defined on a coarse spatial geometry. The counts,
exposure and background map are supposed to be on the same spatial geometry,
chosen by the user for the analysis.

The `MapDataset` implements at least the following methods:

.. code::

    dataset = MapDataset()

    # the map dataset might require some precomputations
    dataset.setup()?

    dataset.npred()
    dataset.likelihood_per_bin()
    dataset.likelihood(mask)

The `.setup()` method creates cutouts of the exposure map for the individual
model components, assigns the correspoding IRFs to the model component and
bundles all into `SourceIRFModel` or `ModelEvaluator()` classes. The list of model
evaluators is cached on the `MapDataset` and used later to efficiently compute
the predicted number of counts.

The `mask` argument to the `.likelihood` method allows to set a global fitting
mask, when mutiple `MapDataset` are fitted jointly.


`MapDatasetOnOff`
----------------
For on-off based analyses a `MapDatasetOnOff` class could be introduced.

.. code ::

    from gammapy.cube import MapDatasetOnOff

    model = SourceModel.read("model.yaml")

    counts_on = Map.read("stacked/counts_on.fits")
    counts_off = Map.read("stacked/counts_off.fits")
    alpha = Map.read("stacked/alpha.fits")

    edisp = Map.read("stacked/edisp.fits")
    psf = Map.read("stacked/psf.fits")

    dataset_onoff = MapDatasetOnOff(
        counts_on=counts_on,
        counts_off=counts_off,
        alpha=alpha,
        edisp=edisp,
        psf=psf,
        mask=mask,
        model=model,
        likelihood="wstat"
        )

    fit = Fit(dataset)
    fit.optimize()


`SpectrumDataset`
-----------------

For spectral analysis we propose to introduce a `SpectrumDataset`:

.. code ::

    from gammapy.spectrum import SpectrumDataset

    model = SpectralModel()
    background_model = SpectralBackgroundModel.read()
    edisp = EnergyDispersion.read()

    counts = Spectrum.read()
    exposure = Spectrum.read()
    mask = Spectrum.read()

    dataset = SpectrumDataset(
        counts=counts,
        exposure=exposure,
        model=model,
        background_model=background_model,
        mask=mask,
        edisp=edisp,
        likelihood="cash"
        )

    dataset.npred()
    dataset.likelihoop_per_bin()
    dataset.likelihood(mask)

The `SpectrumDatasets` with a parametric background model will be introduced first.
The existing `SpectrumObservation` can be refactored later in a `SpectrumOnOffDataset`
or `PHASpectrumDatasets`.


`FluxPointDataset`
------------------

For modeling of flux points we propose to introduce a `FluxPointDataset`:

.. code::

    from gammapy.spectrum import FluxPointsDataset

    flux_points = FluxPoints.read("flux_points.ecsv")
    model = PowerLaw()
    dataset = FluxPointsDataset(
        flux_points=flux_points,
        model=model,
        likelihood="chi2"
        )

    fit = Fit(dataset)
    fit.optimize()

The `FluxPointDataset` should define at least the following methods:

.. code::

    dataset = FluxPointDataset()

    dataset.likelihood_per_bin()
    dataset.likelihood(mask)

    # predicted flux form the spectral model
    dataset.flux_pred()

The `FluxPoint` class also supports the `likelihood` format, which has to be implemented
in a special way in the `FluxPointDataset`. The `likelihood` format stores the
likelihood of the flux point, depending on energy and amplitude. Given a predicted
flux by the spectral model the likelihood can be directly interpolated. This could
be supported by a specific option `FluxPointDataset(likelihood="template")` or similar.


`Datasets`
----------

To combined multiple datasets in a joint-likelihood analysis we propose to introduce
a `Datasets` class. It's repsonsibility is to add up the log-likelihood values of
the individual datasets and join per dataset parameter lists. The `Datasets` class
also represents the main interface to the `Fit` class.

The following example shows how a joint-likelihood analysis of mutiple observations
is supposed to work:

.. code ::

    from gammapy.utils.fitting import Datatsets

    model = SourceModel.read()

    map_datasets = []

    for obs_id in [123, 124, ...]:
        bkg_model = BackgroundModel.read("obs-{}/background.fits".format(obs_id))
        dataset = MapDataset(model=model, background=bkg_model)
        map_datasets.append(dataset)

    datasets = Datasets(map_datasets)

    # check whether all contained datasets are of the same type
    datasets.is_all_same_type

    datasets.likelihood(mask)

    fit = Fit(datasets)

    # or maybe simpler

    fit = Fit(map_datasets)
    fit.optimize()



The linking of parameters of the spectral model is natively achieved, as the same
instance of the spectral model is passed to two different datasets, while the
background model is created for every dataset separately.

A joined spectral fit across multiple observations:

.. code ::

    model = SpectralModel()

    spectrum_dataset_1 = SpectrumDataset(model, )
    spectrum_dataset_2 = SpectrumDataset(model, )

    datasets = Datasets([spectrum_dataset_1, spectrum_dataset_2])

    fit = Fit(datasets)
    fit.optimize()


Combined spectral / flux points analysis:

.. code ::

    model = SpectralModel()

    spectrum_dataset = SpectrumDataset(model=model)

    flux_points = FluxPoints.read()
    flux_point_dataset = FluxPointDataset(flux_points=flux_points, model=model, likelihood="chi2")

    datasets = Datasets([flux_point_dataset, spectrum_dataset])

    fit = Fit(datasets)
    fit.optimize()

Parallel evaluation of datasets
-------------------------------

For efficient joint-likelihood fits of multiple observations, parallel processing
should be used. The obvious entry point is to evaluate one dataset per process and
join the likelihoods at the end. The current handling of model references
(the same model instance is passed to different datasets to achieve a generic
parameter linking by updating the model parameters in place), set's a limitation
on the parallel evaluation, because Python objects can't easily be shared between
multiple processes. We are aware of this issue, but propose to solve it later,
because it does not affect the proposed API for datasets.


Dataset serialization
---------------------

For convenience all dataset classes should support serialization, implemented
via `.read()` and `.write()` methods. As the dataset as to orchestrate the
serialization of mutiple objects, such as model, maps, flux-points etc. the best
option is likely to introduce the serialization with a YAML based index file:

.. code ::

    dataset = MapDataset.read("dataset.yaml") # lazy loading?
    dataset.write("dataset.yaml")

Where the index file points to the various files needed for initialization of the
dataset. Here is an example:

.. code::

    dataset:
        type: map-dataset
        counts: "obs-123/counts.fits"
        exposure: "obs-123/exposure.fits"
        edisp: "obs-123/edisp.fits"
        psf: "obs-123/psf.fits"
        model: "model.yaml"
        background-model: "obs-123/background.fits"

Addtionally one could introduce a single FITS file serializiaton for quickly writing /
reading datasets to disk.

The `Datasets` object could be serialized equivalently as a list of datasets:

.. code::

    - dataset:
        type: spectrum-dataset
        maps:
            counts: "obs-123/counts.fits"
            exposure: "obs-123/exposure.fits"
            edisp: "obs-123/edisp.fits"
        model: "model.yaml"
        background-model: "obs-123/background.fits"
        likelihood: "wstat"

    - dataset:
        type: spectrum-dataset
        maps:
            counts: "obs-124/counts.fits"
            exposure: "obs-124/exposure.fits"
            edisp: "obs-124/edisp.fits"
        model: "model.yaml"
        background-model: "obs-124/background.fits"
        likelihood: "wstat"

    - dataset:
        type: flux-point-dataset
        flux-points : "fermipy-flux-points.fits"
        model: "spectral-model.yaml"
        likelihood: "template"


Lazy loading of Datasets
------------------------
For the use-case of inspecting individual datasets (e.g. MapDataset per observation)
it could be advantegeous to implement a lazy-loading or generator interface for
`Datasets.read()`. Such that the individual datasets are only read from disk on
requests and are not loaded in memory when calling `.read()`. We propose to handle
this with a generic lazy-loading mechanism for `Map` objects, where the data is loaded
on access of the `.data` attribute.


Simulation of `MapDataset`
--------------------------
The `MapDataset` class will be the central class for map based analyses. In many
cases it's useful to simulate counts from a predicted number of counts model. Two
approaches are possible: simulating a counts map by calling `np.random.poisson()` on
a predicted number of counts map or sampling of event lists. The latter is addressed
in a separate PIG (#009). The direct sampling of binned data could be supported
as following:

.. code::

    # allow counts=None on init
    dataset = MapDataset(counts=None)

    # to sample a counts map from npred
    dataset.counts = dataset.simulate_counts(random_seed=0)

    fit = Fit(dataset)
    fit.optmimize()


List of Pull Requests
=====================

This is a proposal for a list of pull requests implementing the proposed changes,
ordered by priority:

1. Refactor the current `FluxPointFit` into a `FluxPointsDataset` class
and make it work with the current `Fit` class. Ensure that the `Fit` class
supports the old inheritance scheme as well as the new dataset on init.
Update tests and tutorials (**v0.11**).

2. Refactor the current `MapFit` into the `MapDataset` class. Only support a
fixed energy dispersion and psf first. Use the current `MapEvaluator` for model
evaluation, but split out the background evaluation. Update test and tutorials (**v0.11**)

3. Implement the `Datasets` class in `gammapy.utils.fitting.datasets`, add tests
using multiple `MapDataset`. Change the `Fit` interface to take a `Datasets`
object or list of `MapDataset` on input (**v0.11**).

4. Add a `name` attribute to datasets and a `Datasets.__getitem__` method (**v0.11**).

5. Enable joint-likelihood analyses by filtering the `Datasets.parameters` for
unique parameters only. Add a tutorial for joint-likelihood fitting of multiple
observations (**v0.11**).

6. Implement `MapDataset.setup()` method, using a list of `MapEvaluator`
objects. Add an `.evaluation_radius()` attribute to all the spatial models.
Only support a fixed PSF and edisp per model component (**v0.11**).

7. Add support for psf maps to `MapDataset.setup()`. Extend the `.setup()` method
to look up the correct PSF for the given model component. A `PSFMap` has to be
passed on `MapDataset.__init__()` (**v0.11**).

8. Add support for energy dispersion maps to `MapDataset`. Extend the `.setup()`
method to look up the correct EDisp for a given model component. An `EdispMap`
has to be passed on `MapDataset.__init__()` (**v0.11**).

9. Add tutorial for joint-likelihood fitting of IACT and Fermi-LAT data, based
on the joint-crab dataset (**v0.12**).

10. Implement dataset serialization to yaml (**v0.12**).

11. Implement datasets serialization to a single fits file (**v0.12**).

12. Add the direct likelihood evaluation to the `FluxPointsDataset`, by
interpolating flux point likelihood profiles (**v0.12**).

13.

Alternatives
============
Joint-likelihood analyses for multiple instruments are already possible using
the 3ML Python package. Alternatively Gammapy could provide a plugin for the
3ML software and enable the joint-likelihood or joint-instrument analyses for
users this way. In this scenario, because joint-likelihood fitting is a required
feature of the future CTA science tools, 3ML would have to be shiped along with
CTA science tools. We explore the possibilities of collaborating closer with 3ML.


Decision
========



.. _gammapy: https://github.com/gammapy/gammapy
.. _gammapy-web: https://github.com/gammapy/gammapy-webpage